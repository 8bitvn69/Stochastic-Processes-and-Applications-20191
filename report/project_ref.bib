% Encoding: UTF-8

@Book{Foster1995,
  author    = {Foster, Ian},
  title     = {Designing and Building Parallel Programs: Concepts and Tools for Parallel Software Engineering},
  year      = {1995},
  publisher = {Addison-Wesley Longman Publishing Co., Inc.},
  isbn      = {0201575949},
  address   = {Boston, MA, USA},
}

@Article{Watkins1992,
author="Watkins, Christopher J. C. H.
and Dayan, Peter",
title="Q-learning",
journal="Machine Learning",
year="1992",
month="May",
day="01",
volume="8",
number="3",
pages="279--292",
abstract="Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.",
issn="1573-0565",
doi="10.1007/BF00992698",
url="https://doi.org/10.1007/BF00992698"
}
@article{Watkins1989,
author = {Watkins, Christopher},
year = {1989},
month = {01},
pages = {},
title = {Learning From Delayed Rewards}
}

@Article{Belman1957,
  author   = {Richard Bellman},
  title    = {A Markovian Decision Process},
  journal  = {Indiana Univ. Math. J.},
  year     = {1957},
  volume   = {6},
  pages    = {679--684},
  issn     = {0022-2518},
  coden    = {IUMJAB},
  fjournal = {Indiana University Mathematics Journal},
  issue    = {4},
}

@Book{Howard1960,
  title     = {Dynamic programming and Markov processes},
  publisher = {Massachusetts Institute of Technology P.;Wiley},
  year      = {1960},
  author    = {Howard, Ronald A},
}

@Book{Bertsekas2005,
  title     = {Dynamic Programming \& Optimal Control},
  publisher = {Athena Scientific; 3 edition (1600)},
  year      = {2005},
  author    = {Dimitri P. Bertsekas},
}

@Book{Puterman1994,
  title     = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  publisher = {John Wiley \& Sons},
  year      = {1994},
  author    = {Martin L. Puterman},
}

@Article{Sutton1999,
  author  = {Sutton, Richard and Barto, AG},
  title   = {Reinforcement learning},
  journal = {Journal of Cognitive Neuroscience},
  year    = {1999},
  volume  = {11},
  pages   = {126-134},
  month   = {01},
}

@Book{Gagniuc2017,
  title     = {Markov Chains: From Theory to Implementation and Experimentation},
  publisher = {John Wiley \& Sons},
  year      = {2017},
  author    = {Paul A. Gagniuc},
  isbn      = {978-1-119-38755-8},
}

@Article{White1969,
  author  = {White, DJ},
  title   = {Dynamic Programming},
  journal = {San Francisco HoldenDay},
  year    = {1969},
}
@Article{Andreae1969b,
  author  = {Andreae, J. H},
  title   = {Learning machines - a unied view},
  journal = {Encyclopedia of Information Linguistics and Control},
  year    = {1969},
  pages   = {261 - 270},
}

@Article{Witten1973,
  author  = {Witten, IH and Corbin, MJ},
  title   = {Human operators and automatic adaptive controllers: A comparative study on a particular control task},
  journal = {International Journal of ManMachine Studies},
  year    = {1973},
  volume  = {5},
  pages   = {75 - 104},
}

@Article{Witten1977,
  author  = {Witten, IH},
  title   = {An adaptive optimal controller for discretetime Markov environments},
  journal = {Information and Control},
  year    = {1977},
  pages   = {286 - 295},
}

@Article{Werbos1977,
  author  = {Werbos, PJ},
  title   = {Advanced forecasting methods for global crisis warning and models of intelligence},
  journal = {General Systems Yearbook},
  year    = {1977},
  pages   = {25 - 38},
}

@Comment{jabref-meta: databaseType:biblatex;}
